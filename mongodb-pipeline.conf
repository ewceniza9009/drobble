# mongodb-pipeline.conf

input {
  mongodb {
    uri => "mongodb://localhost:27017/DrobbleProductCatalog"
    collection => "products"
    batch_size => 5000
  }
}

filter {
  # Step 1: Move the original MongoDB _id to the metadata field.
  # This is crucial for setting the document ID in Elasticsearch without conflict.
  mutate {
    rename => { "_id" => "[@metadata][_id]" }
  }

  # Step 2: Convert data types for proper indexing and aggregation.
  # For example, 'Stock' should be a number to perform calculations on it.
  mutate {
    convert => {
      "Stock" => "integer"
      "Price" => "float"
      "IsActive" => "boolean"
      "IsFeatured" => "boolean"
    }
  }

  # Step 3: Parse the 'CreatedAt' date string.
  # This makes the data time-sensitive and usable in Kibana visualizations.
  # We'll use the ISO8601 format, which matches the "$date" value.
  date {
    match => [ "CreatedAt", "ISO8601" ]
    target => "@timestamp" # Overwrites Logstash's default timestamp with your actual creation date.
  }

  # Step 4: Clean up fields you don't need in Elasticsearch.
  # Keeping your index clean improves performance.
  mutate {
    remove_field => [
      "@version", # This is a field Logstash adds by default.
      "UpdatedAt", # Assuming we only care about the creation date.
      "CreatedAt"  # We can remove the original string now that we have @timestamp.
    ]
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "products" # The name of the index in Elasticsearch.
    document_id => "%{[@metadata][_id]}" # Sets the Elasticsearch document ID to the original MongoDB ID.
  }
  
  # Optional: Keep this for debugging while you make changes.
  stdout { codec => rubydebug }
}